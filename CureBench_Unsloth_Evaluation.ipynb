{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2e86f9",
   "metadata": {},
   "source": [
    "# CureBench Evaluation with Unsloth Models\n",
    "\n",
    "This notebook demonstrates how to evaluate Unsloth models on the CureBench dataset in Google Colab.\n",
    "\n",
    "## Overview\n",
    "- Load Unsloth optimized medical models\n",
    "- Run evaluation on CureBench dataset  \n",
    "- Generate competition submission files\n",
    "- Compatible with Google Colab and GitHub deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3a3d2",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "Install Unsloth and other required packages in Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc20875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" trl peft accelerate bitsandbytes\n",
    "\n",
    "# Install other required packages\n",
    "!pip install transformers torch tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce0f70",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Check GPU\n",
    "\n",
    "Import necessary libraries including torch, unsloth, and verify GPU availability in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from unsloth import FastModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔥 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"🔥 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Consider using GPU runtime for better performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65d65c",
   "metadata": {},
   "source": [
    "## 3. Load and Configure the Model\n",
    "\n",
    "Load the medgemma model with proper configuration for Colab environment, fixing any compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f76f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models - choose one that fits your GPU memory\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Medical models\n",
    "    \"unsloth/medgemma-4b-it-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# Choose model based on your needs\n",
    "model_name = \"unsloth/medgemma-4b-it-bnb-4bit\"  # Good for medical tasks\n",
    "# model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\"  # Alternative option\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load model with optimized settings for Colab\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # Enable 4-bit quantization for memory efficiency\n",
    "    full_finetuning = False, # We're doing inference only\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"📊 Model name: {model_name}\")\n",
    "print(f\"📊 Max sequence length: {model.config.max_position_embeddings if hasattr(model.config, 'max_position_embeddings') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939356a7",
   "metadata": {},
   "source": [
    "## 4. Test Model Inference\n",
    "\n",
    "Test the loaded model with sample prompts to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a medical question\n",
    "test_prompt = \"\"\"The following is a multiple choice question about medicine. Answer with only the letter (A, B, C, D, or E).\n",
    "\n",
    "Question: A 65-year-old patient presents with chest pain. Which of the following is the most appropriate initial diagnostic test?\n",
    "A) CT scan\n",
    "B) Electrocardiogram (ECG)\n",
    "C) Blood pressure measurement\n",
    "D) X-ray\n",
    "E) MRI\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Apply chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "print(\"🧠 Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response_tokens = outputs[0][inputs.input_ids.shape[-1]:]\n",
    "response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"✅ Test inference completed!\")\n",
    "print(f\"📝 Question: {test_prompt.split('Question: ')[1].split('Answer:')[0].strip()}\")\n",
    "print(f\"🤖 Model Response: {response}\")\n",
    "\n",
    "# Clean up memory\n",
    "del outputs, inputs\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f7ae4",
   "metadata": {},
   "source": [
    "## 5. Clone CureBench Repository and Setup Evaluation\n",
    "\n",
    "Clone the evaluation framework and prepare for running the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\n",
    "\n",
    "# Navigate to the repository directory\n",
    "import os\n",
    "os.chdir('YOUR_REPO_NAME')  # Replace with your repo name\n",
    "\n",
    "# Install any additional requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print(\"✅ Repository cloned and dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265789ba",
   "metadata": {},
   "source": [
    "## 6. Setup Evaluation Framework\n",
    "\n",
    "Initialize the CureBench evaluation framework with our Unsloth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation framework\n",
    "from eval_framework import CompetitionKit\n",
    "\n",
    "# Create metadata for submission\n",
    "metadata = {\n",
    "    \"model_name\": model_name,\n",
    "    \"model_type\": \"UnslothModel\", \n",
    "    \"track\": \"internal_reasoning\",\n",
    "    \"base_model_type\": \"OpenWeighted\",\n",
    "    \"base_model_name\": model_name.split('/')[-1],\n",
    "    \"dataset\": \"cure_bench_phase_1\",\n",
    "    \"additional_info\": f\"Unsloth optimized model with 4-bit quantization running on Colab\"\n",
    "}\n",
    "\n",
    "# Create dataset configuration\n",
    "dataset_config = {\n",
    "    \"dataset\": {\n",
    "        \"dataset_name\": \"cure_bench_phase_1\",\n",
    "        \"dataset_path\": \"curebench_testset_phase1.jsonl\",  # Update if needed\n",
    "        \"description\": \"CureBench 2025 Phase 1 test questions\"\n",
    "    },\n",
    "    \"output_dir\": \"results\"\n",
    "}\n",
    "\n",
    "# Initialize competition kit\n",
    "kit = CompetitionKit()\n",
    "kit.datasets = {\"cure_bench_phase_1\": dataset_config[\"dataset\"]}\n",
    "\n",
    "# Load the model in the framework\n",
    "kit.load_model(\n",
    "    model_name=model_name,\n",
    "    model_type=\"unsloth\",\n",
    "    model_instance=model,\n",
    "    tokenizer_instance=tokenizer\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation framework initialized!\")\n",
    "print(f\"📊 Model: {model_name}\")\n",
    "print(f\"📊 Dataset: cure_bench_phase_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81267aee",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation\n",
    "\n",
    "Execute the evaluation on the CureBench dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b408309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available datasets\n",
    "print(\"Available datasets:\")\n",
    "kit.list_datasets()\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n🚀 Starting evaluation...\")\n",
    "print(\"This may take several minutes depending on dataset size.\")\n",
    "\n",
    "results = kit.evaluate(\"cure_bench_phase_1\")\n",
    "\n",
    "print(f\"\\n✅ Evaluation completed!\")\n",
    "print(f\"📊 Accuracy: {results.accuracy:.2%}\")\n",
    "print(f\"📊 Correct predictions: {results.correct_predictions}/{results.total_examples}\")\n",
    "print(f\"📊 Total examples processed: {len(results.predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92661cf",
   "metadata": {},
   "source": [
    "## 8. Generate Submission File\n",
    "\n",
    "Create the competition submission file with proper metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40480f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission with metadata\n",
    "print(\"📄 Generating submission file...\")\n",
    "\n",
    "submission_filename = f\"unsloth_submission_{model_name.split('/')[-1]}.csv\"\n",
    "submission_path = kit.save_submission_with_metadata(\n",
    "    results=[results],\n",
    "    metadata=metadata,\n",
    "    filename=submission_filename\n",
    ")\n",
    "\n",
    "print(f\"✅ Submission saved to: {submission_path}\")\n",
    "print(\"📄 Ready for competition submission!\")\n",
    "\n",
    "# Display metadata summary\n",
    "print(\"\\n📋 Submission metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Show some sample predictions\n",
    "print(\"\\n📝 Sample predictions:\")\n",
    "for i, pred in enumerate(results.predictions[:3]):\n",
    "    print(f\"  Example {i+1}:\")\n",
    "    print(f\"    Choice: {pred.get('choice', 'N/A')}\")\n",
    "    print(f\"    Answer: {pred.get('open_ended_answer', 'N/A')[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee836ef4",
   "metadata": {},
   "source": [
    "## 9. Download Results (Optional)\n",
    "\n",
    "Download the submission file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e173e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the submission file in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"📥 Downloading submission file...\")\n",
    "    files.download(submission_path)\n",
    "    print(\"✅ Download completed!\")\n",
    "except ImportError:\n",
    "    print(\"ℹ️ Not running in Google Colab. Submission file is available at:\", submission_path)\n",
    "\n",
    "# List all files in results directory\n",
    "import os\n",
    "print(\"\\n📁 Files in results directory:\")\n",
    "for file in os.listdir(\"results\"):\n",
    "    file_path = os.path.join(\"results\", file)\n",
    "    file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "    print(f\"  {file} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3434aa7",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Memory Management\n",
    "\n",
    "Free up GPU memory after evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "print(\"🧹 Cleaning up GPU memory...\")\n",
    "\n",
    "# Delete model and tokenizer\n",
    "del model, tokenizer\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Cleanup completed!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔥 GPU memory freed: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB available\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
